{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQkyk6J1O-du",
        "outputId": "c7ee600f-ca35-443d-bedd-e43ab6731e32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "9iOx8a35Rn71"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "def shuffle_list(list):\n",
        "  shuffled = list.copy()\n",
        "  random.shuffle(shuffled)\n",
        "\n",
        "  return shuffled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2JtL5wuMRpRL"
      },
      "outputs": [],
      "source": [
        "from nltk import RegexpTokenizer\n",
        "\n",
        "toknizer = RegexpTokenizer(r'''\\w+'|[\\w-]+|[^\\w\\s]''')\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return toknizer.tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "gnNOIM8PRvNG"
      },
      "outputs": [],
      "source": [
        "def get_vocab(sequences):\n",
        "  token_id_map = {\n",
        "      \"<pad>\": 0,\n",
        "      \"<start>\": 1,\n",
        "      \"<stop>\": 2\n",
        "  }\n",
        "\n",
        "  for sequence in sequences:\n",
        "    for word in sequence:\n",
        "      if word not in token_id_map:\n",
        "        token_id_map[word] = len(token_id_map)\n",
        "\n",
        "  return token_id_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ox8xG0rwRwNQ"
      },
      "outputs": [],
      "source": [
        "def encode_with_vocab(sequences, vocab):\n",
        "  encoded_sequences = []\n",
        "  encoded_sequence = []\n",
        "\n",
        "  for sequence in sequences:\n",
        "    for word in sequence:\n",
        "      encoded_sequence.append(vocab[word])\n",
        "    encoded_sequences.append(encoded_sequence)\n",
        "    encoded_sequence = []\n",
        "\n",
        "  return encoded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "m1lC1aH8Ney8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_dataset(name):\n",
        "  if (name == \"fr-en\"):\n",
        "    f = open(\"gdrive/MyDrive/Data/fr-en-train.txt\", \"r\")\n",
        "    file = f.read()\n",
        "    return file.split(\"\\n\")\n",
        "  else:\n",
        "    raise SystemError(\"Dataset not found\")\n",
        "\n",
        "def prepare_dataset(dataset, shuffle, lowercase, max_window_size):\n",
        "  encoder_input, decoder_input, decoder_output = [], [], []\n",
        "  encoder_vocab, decoder_vocab, encoder_inverted_vocab, decoder_inverted_vocab = {}, {}, {}, {}\n",
        "\n",
        "  if shuffle:\n",
        "    dataset = shuffle_list(dataset)\n",
        "\n",
        "  for line in dataset:\n",
        "    if lowercase:\n",
        "      line = line.lower()\n",
        "    en, fr, credits = line.split(\"\\t\")\n",
        "\n",
        "    encoder_input.append(tokenize(fr))\n",
        "    decoder_input.append(tokenize(en))\n",
        "\n",
        "  decoder_output = [tokens + [\"<stop>\"] for tokens in decoder_input]\n",
        "  encoder_input = [[\"<start>\"] + tokens + [\"<stop>\"] for tokens in encoder_input]\n",
        "  decoder_input = [[\"<start>\"] + tokens + [\"<stop>\"] for tokens in decoder_input]\n",
        "\n",
        "  source_max_len = max_window_size\n",
        "  target_max_len = max_window_size\n",
        "  if (max(map(len, encoder_input)) > max_window_size or max(map(len, decoder_input)) > max_window_size):\n",
        "    raise SystemError(\"Maximum window size is too small\", max(map(len, encoder_input)), max(map(len, decoder_input)))\n",
        "\n",
        "  encoder_input = [tokens + [\"<pad>\"] * (source_max_len - len(tokens)) for tokens in encoder_input]\n",
        "  decoder_input = [tokens + [\"<pad>\"] * (target_max_len - len(tokens)) for tokens in decoder_input]\n",
        "  decoder_output = [tokens + [\"<pad>\"] * (target_max_len - len(tokens)) for tokens in decoder_output]\n",
        "\n",
        "  encoder_vocab = get_vocab(encoder_input)\n",
        "  decoder_vocab = get_vocab(decoder_input)\n",
        "\n",
        "  encoder_inverted_vocab = { v: k for k, v in encoder_vocab.items() }\n",
        "  decoder_inverted_vocab = { v: k for k, v in decoder_vocab.items() }\n",
        "\n",
        "  encoder_input = encode_with_vocab(encoder_input, encoder_vocab)\n",
        "  decoder_input = encode_with_vocab(decoder_input, decoder_vocab)\n",
        "  decoder_output = encode_with_vocab(decoder_output, decoder_vocab)\n",
        "  decoder_output = [[[token] for token in tokens] for tokens in decoder_output]\n",
        "\n",
        "  return (encoder_input,\n",
        "    decoder_input,\n",
        "    decoder_output,\n",
        "    encoder_vocab,\n",
        "    decoder_vocab,\n",
        "    encoder_inverted_vocab,\n",
        "    decoder_inverted_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "TnUvFTfbN9Cn"
      },
      "outputs": [],
      "source": [
        "def mask_zero(x):\n",
        "    mask = tf.greater(x, 0)\n",
        "    mask = tf.cast(mask, dtype = tf.float32)\n",
        "    return mask\n",
        "\n",
        "class WordEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, embedding_size, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "\n",
        "    config.update({\n",
        "      'vocab_size': self.vocab_size,\n",
        "      'embedding_size': self.embedding_size\n",
        "    })\n",
        "\n",
        "    return config\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super().build(input_shape)\n",
        "\n",
        "    self.word_embedding = tf.keras.layers.Embedding(\n",
        "      self.vocab_size,\n",
        "      self.embedding_size\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    word_embedding = self.word_embedding(x)\n",
        "\n",
        "    window_dim = x.get_shape().as_list()[1]\n",
        "    masks = tf.keras.layers.Lambda(mask_zero, output_shape=(-1,))(x)\n",
        "    masks = tf.keras.layers.Reshape(target_shape=(-1, 1))(masks)\n",
        "    word_embedding = tf.keras.layers.Multiply()([word_embedding, masks])\n",
        "\n",
        "    return word_embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "gHcKuPxcOFkI"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, nb_head, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    if not embedding_size % nb_head == 0:\n",
        "      raise SystemError(\"Embedding_size should be divisible by number of heads\")\n",
        "\n",
        "    self.embedding_size = embedding_size\n",
        "    self.nb_head = nb_head\n",
        "    self.head_dim = embedding_size // nb_head\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "\n",
        "    config.update({\n",
        "      'embedding_size': self.embedding_size,\n",
        "      'nb_head': self.nb_head\n",
        "    })\n",
        "\n",
        "    return config\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super().build(input_shape)\n",
        "\n",
        "    self.query_layer = tf.keras.layers.Dense(self.embedding_size)\n",
        "    self.value_layer = tf.keras.layers.Dense(self.embedding_size)\n",
        "    self.key_layer = tf.keras.layers.Dense(self.embedding_size)\n",
        "    self.out_proj = tf.keras.layers.Dense(self.embedding_size)\n",
        "\n",
        "  def call(self, x, mask = False):\n",
        "    Q_input, K_input, V_input = x\n",
        "\n",
        "    Q = self.query_layer(Q_input)\n",
        "    K = self.key_layer(K_input)\n",
        "    V = self.value_layer(V_input)\n",
        "\n",
        "    if self.nb_head > 1:\n",
        "      batch_size = tf.shape(Q)[0]\n",
        "      Q_seq_len = tf.shape(Q)[1]\n",
        "      K_seq_len = tf.shape(K)[1]\n",
        "      V_seq_len = tf.shape(V)[1]\n",
        "\n",
        "      Q = tf.reshape(Q, [batch_size, Q_seq_len, self.nb_head, self.head_dim])\n",
        "      K = tf.reshape(K, [batch_size, K_seq_len, self.nb_head, self.head_dim])\n",
        "      V = tf.reshape(V, [batch_size, V_seq_len, self.nb_head, self.head_dim])\n",
        "\n",
        "      Q = tf.transpose(Q, [0, 2, 1, 3])\n",
        "      K = tf.transpose(K, [0, 2, 1, 3])\n",
        "      V = tf.transpose(V, [0, 2, 1, 3])\n",
        "\n",
        "      Q = tf.reshape(Q, [batch_size * self.nb_head, Q_seq_len, self.head_dim])\n",
        "      K = tf.reshape(K, [batch_size * self.nb_head, K_seq_len, self.head_dim])\n",
        "      V = tf.reshape(V, [batch_size * self.nb_head, V_seq_len, self.head_dim])\n",
        "\n",
        "    dot_product = tf.matmul(Q, K, transpose_b = True)\n",
        "    scaled_dot_product = dot_product / tf.math.sqrt(float(self.embedding_size))\n",
        "\n",
        "    if mask:\n",
        "      diag_vals = tf.ones_like(scaled_dot_product[0, :, :])\n",
        "      tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n",
        "      future_masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(scaled_dot_product)[0], 1, 1])\n",
        "      padding_num = -float(\"Inf\")\n",
        "      paddings = tf.ones_like(future_masks) * padding_num\n",
        "\n",
        "      scaled_dot_product = tf.where(tf.equal(future_masks, 0), paddings, scaled_dot_product)\n",
        "\n",
        "    softmax_product = tf.nn.softmax(scaled_dot_product, axis = -1)\n",
        "    attention = tf.matmul(softmax_product, V)\n",
        "\n",
        "    if self.nb_head > 1:\n",
        "      attention = tf.reshape(\n",
        "        attention, [batch_size, self.nb_head, Q_seq_len, self.head_dim]\n",
        "      )\n",
        "\n",
        "      attention = tf.transpose(attention, [0, 2, 1, 3])\n",
        "\n",
        "      attention = tf.reshape(\n",
        "        attention, [batch_size, Q_seq_len, self.nb_head * self.head_dim]\n",
        "      )\n",
        "\n",
        "    out_attention = self.out_proj(attention)\n",
        "\n",
        "    return out_attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Hpxh4VKmOAUs"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    input_shape = tf.shape(x)\n",
        "    batch_size, seq_len, output_dim = input_shape[0], input_shape[1], input_shape[2]\n",
        "    pos_input = tf.tile(tf.expand_dims(tf.keras.backend.arange(0, seq_len), axis = 0), [batch_size, 1])\n",
        "    pos_input = tf.keras.backend.cast(pos_input, tf.float32)\n",
        "    evens = tf.keras.backend.arange(0, output_dim // 2) * 2\n",
        "    odds = tf.keras.backend.arange(0, output_dim // 2) * 2 + 1\n",
        "    even_embedding = tf.sin(\n",
        "      tf.keras.backend.dot(\n",
        "        tf.expand_dims(pos_input, -1),\n",
        "        tf.expand_dims(1.0 / tf.pow(\n",
        "          10000.0,\n",
        "          tf.cast(evens, dtype = tf.float32) / tf.cast(output_dim, dtype = tf.float32)\n",
        "          ), 0)\n",
        "        )\n",
        "      )\n",
        "    odd_embedding = tf.cos(\n",
        "      tf.keras.backend.dot(\n",
        "        tf.expand_dims(pos_input, -1),\n",
        "        tf.expand_dims(1.0 / tf.pow(\n",
        "          10000.0,\n",
        "          tf.cast((odds - 1), dtype = tf.float32) / tf.cast(output_dim, dtype = tf.float32)\n",
        "          ), 0)\n",
        "        )\n",
        "      )\n",
        "    embedding = tf.stack([even_embedding, odd_embedding], axis = -1)\n",
        "    output = tf.reshape(embedding, [-1, tf.shape(x)[1], output_dim])\n",
        "    output += x\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "txMJqAE-OMkc"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, dense_layer_size, nb_head, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.embedding_size = embedding_size\n",
        "    self.dense_layer_size = dense_layer_size\n",
        "    self.nb_head = nb_head\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "\n",
        "    config.update({\n",
        "      'embedding_size': self.embedding_size,\n",
        "      'dense_layer_size': self.dense_layer_size,\n",
        "      'nb_head': self.nb_head\n",
        "    })\n",
        "\n",
        "    return config\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super().build(input_shape)\n",
        "\n",
        "    self.attention = MultiHeadAttention(self.embedding_size, self.nb_head)\n",
        "    self.norm_1 = tf.keras.layers.LayerNormalization()\n",
        "    self.norm_2 = tf.keras.layers.LayerNormalization()\n",
        "    self.dense_1 = tf.keras.layers.Dense(self.dense_layer_size)\n",
        "    self.dense_2 = tf.keras.layers.Dense(self.embedding_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    attention = self.attention((x, x, x))\n",
        "    post_attention = self.norm_1(attention + x)\n",
        "\n",
        "    dense_out = self.dense_1(post_attention)\n",
        "    dense_out = self.dense_2(dense_out)\n",
        "\n",
        "    enc_output = self.norm_2(dense_out + x)\n",
        "\n",
        "    return enc_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "NodKLad-OPsz"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, nb_encoder, embedding_size, dense_layer_size, nb_head, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.nb_encoder = nb_encoder\n",
        "    self.embedding_size = embedding_size\n",
        "    self.dense_layer_size = dense_layer_size\n",
        "    self.nb_head = nb_head\n",
        "    self.encoder_layers = []\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "\n",
        "    config.update({\n",
        "      'nb_encoder': self.nb_encoder,\n",
        "      'embedding_size': self.embedding_size,\n",
        "      'dense_layer_size': self.dense_layer_size,\n",
        "      'nb_head': self.nb_head\n",
        "    })\n",
        "\n",
        "    return config\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super().build(input_shape)\n",
        "\n",
        "    for nb in range(self.nb_encoder):\n",
        "      self.encoder_layers.append(\n",
        "        EncoderLayer(self.embedding_size, self.dense_layer_size, self.nb_head)\n",
        "      )\n",
        "\n",
        "  def call(self, x):\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "O0nwIlJoOTv8"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, dense_layer_size, nb_head, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.embedding_size = embedding_size\n",
        "    self.dense_layer_size = dense_layer_size\n",
        "    self.nb_head = nb_head\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "\n",
        "    config.update({\n",
        "      'embedding_size': self.embedding_size,\n",
        "      'dense_layer_size': self.dense_layer_size,\n",
        "      'nb_head': self.nb_head\n",
        "    })\n",
        "\n",
        "    return config\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super().build(input_shape)\n",
        "\n",
        "    self.attention_1 = MultiHeadAttention(self.embedding_size, self.nb_head)\n",
        "    self.attention_2 = MultiHeadAttention(self.embedding_size, self.nb_head)\n",
        "    self.norm_1 = tf.keras.layers.LayerNormalization()\n",
        "    self.norm_2 = tf.keras.layers.LayerNormalization()\n",
        "    self.norm_3 = tf.keras.layers.LayerNormalization()\n",
        "    self.dense_1 = tf.keras.layers.Dense(self.dense_layer_size)\n",
        "    self.dense_2 = tf.keras.layers.Dense(self.embedding_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    output_embedding, encoder_output = x\n",
        "\n",
        "    self_attention = self.attention_1((output_embedding, output_embedding, output_embedding), mask = True)\n",
        "    post_self_attention = self.norm_1(self_attention + output_embedding)\n",
        "\n",
        "    decoder_attention = self.attention_2((post_self_attention, encoder_output, encoder_output))\n",
        "    post_decoder_attention = self.norm_2(decoder_attention + post_self_attention)\n",
        "\n",
        "    dense_out = self.dense_1(post_decoder_attention)\n",
        "    dense_out = self.dense_2(dense_out)\n",
        "\n",
        "    decoder_output = self.norm_3(dense_out + post_decoder_attention)\n",
        "\n",
        "    return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "FJY8-83SOWup"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, nb_decoder, embedding_size, dense_layer_size, nb_head = 1, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.nb_decoder = nb_decoder\n",
        "    self.embedding_size = embedding_size\n",
        "    self.dense_layer_size = dense_layer_size\n",
        "    self.decoder_layers = []\n",
        "    self.nb_head = nb_head\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "\n",
        "    config.update({\n",
        "      'nb_decoder': self.nb_decoder,\n",
        "      'embedding_size': self.embedding_size,\n",
        "      'dense_layer_size': self.dense_layer_size,\n",
        "      'nb_head': self.nb_head\n",
        "    })\n",
        "\n",
        "    return config\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    super().build(input_shape)\n",
        "\n",
        "    for nb in range(self.nb_decoder):\n",
        "      self.decoder_layers.append(\n",
        "        DecoderLayer(self.embedding_size, self.dense_layer_size, self.nb_head)\n",
        "      )\n",
        "\n",
        "  def call(self, x):\n",
        "    output_embedding, encoder_output = x\n",
        "\n",
        "    decoder_output = output_embedding\n",
        "\n",
        "    for decoder_layer in self.decoder_layers:\n",
        "      decoder_output = decoder_layer((decoder_output, encoder_output))\n",
        "\n",
        "    return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "fiPhDQ4TOhau"
      },
      "outputs": [],
      "source": [
        "def get_model(\n",
        "    EMBEDDING_SIZE = 64,\n",
        "    DENSE_LAYER_SIZE = 128,\n",
        "    ENCODER_VOCAB_SIZE = 12,\n",
        "    DECODER_VOCAB_SIZE = 12,\n",
        "    ENCODER_LAYERS = 1,\n",
        "    DECODER_LAYERS = 1,\n",
        "    NUMBER_HEADS = 1\n",
        "  ):\n",
        "\n",
        "  encoder_layer_input = tf.keras.Input(shape = (None,), name = \"Encoder-Input\")\n",
        "  decoder_layer_input = tf.keras.Input(shape = (None,), name = \"Decoder-Input\")\n",
        "\n",
        "  encoder_embedding = WordEmbedding(ENCODER_VOCAB_SIZE, EMBEDDING_SIZE, name = \"Encoder-Word-Embedding\")(encoder_layer_input)\n",
        "  decoder_embedding = WordEmbedding(DECODER_VOCAB_SIZE, EMBEDDING_SIZE, name = \"Decoder-Word-Embedding\")(decoder_layer_input)\n",
        "\n",
        "  encoder_embedding = PositionalEmbedding(name = \"Encoder-Positional-Embedding\")(encoder_embedding)\n",
        "  decoder_embedding = PositionalEmbedding(name = \"Decoder-Positional-Embedding\")(decoder_embedding)\n",
        "\n",
        "  encoder_output = Encoder(ENCODER_LAYERS, EMBEDDING_SIZE, DENSE_LAYER_SIZE, NUMBER_HEADS, name = \"Encoder\")(encoder_embedding)\n",
        "  decoder_output = Decoder(DECODER_LAYERS, EMBEDDING_SIZE, DENSE_LAYER_SIZE, NUMBER_HEADS, name = \"Decoder\")((decoder_embedding, encoder_output))\n",
        "\n",
        "  output_predictions = tf.keras.layers.Dense(DECODER_VOCAB_SIZE, activation = \"softmax\", name = \"Decoder-Output\")(decoder_output)\n",
        "\n",
        "  model = tf.keras.Model([encoder_layer_input, decoder_layer_input], output_predictions, name = \"Transformer-Model\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "c63sE1JZR9bm"
      },
      "outputs": [],
      "source": [
        "def make_translate(model, encoder_vocab, decoder_vocab, decoder_inverted_vocab, max_window_size = 10):\n",
        "  def translate(sentence):\n",
        "    sentence_tokens = [tokens + ['<stop>', '<pad>'] for tokens in [tokenize(sentence)]]\n",
        "    tr_input = [list(map(lambda x: encoder_vocab[x], tokens)) for tokens in sentence_tokens][0]\n",
        "\n",
        "    prediction = [[1]]\n",
        "    i = 0\n",
        "\n",
        "    while int(prediction[0][-1]) is not decoder_vocab['<stop>'] and i < max_window_size + 2:\n",
        "      prediction_auto = model.predict([np.array([tr_input]), np.array(prediction)])\n",
        "      prediction[0].append(tf.argmax(prediction_auto[0][i], axis = -1).numpy())\n",
        "      i += 1\n",
        "\n",
        "    print('Original: {}'.format(sentence))\n",
        "    print('Traduction: {}'.format(' '.join(map(lambda x: decoder_inverted_vocab[x], prediction[0][1:-1]))))\n",
        "\n",
        "  return translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "abvxTRiSO0-m",
        "outputId": "0fc514fa-f4af-494d-e196-488b4c7ff316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Length: 185583 lines\n",
            "Train data loaded. Length: 100000 lines\n",
            "Model: \"Transformer-Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Encoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Encoder-Word-Embedding (WordEm  (None, None, 64)    1110016     ['Encoder-Input[0][0]']          \n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " Decoder-Word-Embedding (WordEm  (None, None, 64)    579456      ['Decoder-Input[0][0]']          \n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " Encoder-Positional-Embedding (  (None, None, 64)    0           ['Encoder-Word-Embedding[0][0]'] \n",
            " PositionalEmbedding)                                                                             \n",
            "                                                                                                  \n",
            " Decoder-Positional-Embedding (  (None, None, 64)    0           ['Decoder-Word-Embedding[0][0]'] \n",
            " PositionalEmbedding)                                                                             \n",
            "                                                                                                  \n",
            " Encoder (Encoder)              (None, None, 64)     66944       ['Encoder-Positional-Embedding[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " Decoder (Decoder)              (None, None, 64)     100480      ['Decoder-Positional-Embedding[0]\n",
            "                                                                 [0]',                            \n",
            "                                                                  'Encoder[0][0]']                \n",
            "                                                                                                  \n",
            " Decoder-Output (Dense)         (None, None, 9054)   588510      ['Decoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,445,406\n",
            "Trainable params: 2,445,406\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/15\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 0.9790\n",
            "Epoch 1: val_accuracy improved from -inf to 0.97877, saving model to ./gdrive/MyDrive/transformer_ep-01_loss-0.24_acc-0.98.ckpt\n",
            "2813/2813 [==============================] - 286s 93ms/step - loss: 0.2357 - accuracy: 0.9790 - val_loss: 0.1253 - val_accuracy: 0.9788\n",
            "Epoch 2/15\n",
            "2062/2813 [====================>.........] - ETA: 1:00 - loss: 0.0694 - accuracy: 0.9876"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-cac3f1f532c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m transformer_model.fit(\n\u001b[0m\u001b[1;32m     72\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dataset = get_dataset(\"fr-en\")\n",
        "\n",
        "print(\"Dataset loaded. Length:\", len(dataset), \"lines\")\n",
        "\n",
        "train_dataset = dataset[0:100000]\n",
        "\n",
        "print(\"Train data loaded. Length:\", len(train_dataset), \"lines\")\n",
        "\n",
        "(encoder_input,\n",
        "decoder_input,\n",
        "decoder_output,\n",
        "encoder_vocab,\n",
        "decoder_vocab,\n",
        "encoder_inverted_vocab,\n",
        "decoder_inverted_vocab) = prepare_dataset(\n",
        "  train_dataset,\n",
        "  shuffle = False,\n",
        "  lowercase = True,\n",
        "  max_window_size = 200\n",
        ")\n",
        "\n",
        "transformer_model = get_model(\n",
        "  EMBEDDING_SIZE = 64,\n",
        "  ENCODER_VOCAB_SIZE = len(encoder_vocab),\n",
        "  DECODER_VOCAB_SIZE = len(decoder_vocab),\n",
        "  ENCODER_LAYERS = 2,\n",
        "  DECODER_LAYERS = 2,\n",
        "  NUMBER_HEADS = 4,\n",
        "  DENSE_LAYER_SIZE = 128\n",
        ")\n",
        "\n",
        "transformer_model.compile(\n",
        "  optimizer = \"adam\",\n",
        "  loss = [\n",
        "    \"sparse_categorical_crossentropy\"\n",
        "  ],\n",
        "  metrics = [\n",
        "    \"accuracy\"\n",
        "  ]\n",
        ")\n",
        "\n",
        "transformer_model.summary()\n",
        "\n",
        "x = [np.array(encoder_input), np.array(decoder_input)]\n",
        "y = np.array(decoder_output)\n",
        "\n",
        "name = \"transformer\"\n",
        "checkpoint_filepath = \"./gdrive/MyDrive/transformer_ep-{epoch:02d}_loss-{loss:.2f}_acc-{accuracy:.2f}.ckpt\"\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "  log_dir = \"logs/{}\".format(name)\n",
        ")\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath = checkpoint_filepath,\n",
        "  monitor = \"val_accuracy\",\n",
        "  mode = \"max\",\n",
        "  save_weights_only = True,\n",
        "  save_best_only = True,\n",
        "  verbose = True\n",
        ")\n",
        "\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor = \"val_accuracy\",\n",
        "  mode = \"max\",\n",
        "  patience = 2,\n",
        "  min_delta = 0.001,\n",
        "  verbose = True\n",
        ")\n",
        "\n",
        "transformer_model.fit(\n",
        "  x,\n",
        "  y,\n",
        "  epochs = 15,\n",
        "  batch_size = 32,\n",
        "  validation_split = 0.1,\n",
        "  callbacks=[\n",
        "    model_checkpoint_callback,\n",
        "    tensorboard_callback,\n",
        "    early_stopping_callback\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST9wpqVzO7PB"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[0:100000]\n",
        "\n",
        "print(\"Train data loaded. Length:\", len(train_dataset), \"lines\")\n",
        "\n",
        "(encoder_input,\n",
        "decoder_input,\n",
        "decoder_output,\n",
        "encoder_vocab,\n",
        "decoder_vocab,\n",
        "encoder_inverted_vocab,\n",
        "decoder_inverted_vocab) = prepare_dataset(\n",
        "  train_dataset,\n",
        "  shuffle = False,\n",
        "  lowercase = True,\n",
        "  max_window_size = 20\n",
        ")\n",
        "\n",
        "transformer_model = get_model(\n",
        "  EMBEDDING_SIZE = 64,\n",
        "  ENCODER_VOCAB_SIZE = len(encoder_vocab),\n",
        "  DECODER_VOCAB_SIZE = len(decoder_vocab),\n",
        "  ENCODER_LAYERS = 2,\n",
        "  DECODER_LAYERS = 2,\n",
        "  NUMBER_HEADS = 4,\n",
        "  DENSE_LAYER_SIZE = 128\n",
        ")\n",
        "\n",
        "transformer_model.summary()\n",
        "\n",
        "transformer_model.load_weights('./gdrive/MyDrive/transformer_ep-0.ckpt')\n",
        "\n",
        "translate = make_translate(transformer_model, encoder_vocab, decoder_vocab, decoder_inverted_vocab)\n",
        "\n",
        "translate(\"c'est une belle journée .\")\n",
        "translate(\"j'aime manger du gâteau .\")\n",
        "translate(\"c'est une bonne chose .\")\n",
        "translate(\"il faut faire à manger pour nourrir les gens .\")\n",
        "translate(\"tom a acheté un nouveau vélo .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZASZVqbD7BK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "translate(\"est a bon exemple est a bon exemple est a bon exemple est a bon exemple est a bon exemple. est a bon exemple est a bon exemple est a bon exemple est a bon exemple est a bon exemple.\")\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ABURWVnOoWT"
      },
      "outputs": [],
      "source": [
        "from tensorboard.plugins.hparams import api as hp\n",
        "dataset = get_dataset(\"fr-en\")\n",
        "\n",
        "train_dataset = dataset[0:150]\n",
        "\n",
        "(encoder_input,\n",
        "decoder_input,\n",
        "decoder_output,\n",
        "encoder_vocab,\n",
        "decoder_vocab,\n",
        "encoder_inverted_vocab,\n",
        "decoder_inverted_vocab) = prepare_dataset(\n",
        "  train_dataset,\n",
        "  shuffle = True,\n",
        "  lowercase = True,\n",
        "  max_window_size = 20\n",
        ")\n",
        "\n",
        "x_train = [np.array(encoder_input[0:100]), np.array(decoder_input[0:100])]\n",
        "y_train = np.array(decoder_output[0:100])\n",
        "\n",
        "x_test = [np.array(encoder_input[100:150]), np.array(decoder_input[100:150])]\n",
        "y_test = np.array(decoder_output[100:150])\n",
        "\n",
        "BATCH_SIZE = hp.HParam(\"batch_num\", hp.Discrete([32, 16]))\n",
        "DENSE_NUM = hp.HParam(\"dense_num\", hp.Discrete([512, 256]))\n",
        "HEAD_NUM = hp.HParam(\"head_num\", hp.Discrete([8, 4]))\n",
        "EMBED_NUM = hp.HParam(\"embed_num\", hp.Discrete([512, 256]))\n",
        "LAYER_NUM = hp.HParam(\"layer_num\", hp.Discrete([6, 4]))\n",
        "\n",
        "with tf.summary.create_file_writer(\"logs/hparam_tuning\").as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[LAYER_NUM, HEAD_NUM, EMBED_NUM, DENSE_NUM, BATCH_SIZE],\n",
        "    metrics=[\n",
        "      hp.Metric(\"val_accuracy\")\n",
        "    ],\n",
        "  )\n",
        "\n",
        "def train_test_model(hparams):\n",
        "  transformer_model = get_model(\n",
        "    EMBEDDING_SIZE = hparams[EMBED_NUM],\n",
        "    ENCODER_VOCAB_SIZE = len(encoder_vocab),\n",
        "    DECODER_VOCAB_SIZE = len(decoder_vocab),\n",
        "    ENCODER_LAYERS = hparams[LAYER_NUM],\n",
        "    DECODER_LAYERS = hparams[LAYER_NUM],\n",
        "    NUMBER_HEADS = hparams[HEAD_NUM],\n",
        "    DENSE_LAYER_SIZE = hparams[DENSE_NUM]\n",
        "  )\n",
        "\n",
        "  transformer_model.compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = [\"sparse_categorical_crossentropy\"],\n",
        "    metrics = [\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  transformer_model.fit(x_train, y_train, epochs = 1, batch_size = hparams[BATCH_SIZE])\n",
        "\n",
        "  _, accuracy = transformer_model.evaluate(x_test, y_test)\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)\n",
        "    accuracy = train_test_model(hparams)\n",
        "    tf.summary.scalar(\"val_accuracy\", accuracy, step = 1)\n",
        "\n",
        "session_num = 0\n",
        "\n",
        "for batch_num in BATCH_SIZE.domain.values:\n",
        "  for dense_num in DENSE_NUM.domain.values:\n",
        "    for num_heads in HEAD_NUM.domain.values:\n",
        "      for num_embed in EMBED_NUM.domain.values:\n",
        "        for num_units in LAYER_NUM.domain.values:\n",
        "          hparams = {\n",
        "              BATCH_SIZE: batch_num,\n",
        "              DENSE_NUM: dense_num,\n",
        "              HEAD_NUM: num_heads,\n",
        "              EMBED_NUM: num_embed,\n",
        "              LAYER_NUM: num_units\n",
        "          }\n",
        "          run_name = \"run-%d\" % session_num\n",
        "\n",
        "          print(\"--- Starting trial: %s\" % run_name)\n",
        "          print({ h.name: hparams[h] for h in hparams })\n",
        "\n",
        "          run(\"logs/hparam_tuning/\" + run_name, hparams)\n",
        "\n",
        "          session_num += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}